{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1c2tK-_d41vZ8pf6cxfAMderzGLJ6BFst","timestamp":1644887814780}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"kp4bUGnqpGzs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665953036010,"user_tz":240,"elapsed":19569,"user":{"displayName":"Yuewei Yuan","userId":"03266189347381662209"}},"outputId":"64df0107-29b1-4842-ceac-b9503ab9716e"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","data_path = '/content/drive/My Drive/AmbiCoref/AmbiCoref/Data/'"],"metadata":{"id":"xidhCzQbCL2c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9Z_OFdtpATL"},"source":["# Data"]},{"cell_type":"markdown","source":["## Gendered names and noun phrases \n","\n","\n","*   20 gendered phrases: \n","  * partially from https://github.com/uclanlp/corefBias/blob/master/WinoBias/wino/extra_gendered_words.txt\n","*   20 gendered names: \n","  * https://www.ssa.gov/oact/babynames/decades/names1970s.html\n","  * most popular names from the past 5 decades; 4 names sampled for each decade.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"4aCEcqaacLQ5"}},{"cell_type":"code","source":["names_df = pd.read_csv(data_path + 'NPs/Names.csv')\n","NPs_df = pd.read_csv(data_path + 'NPs/Gendered_NPs.csv')\n","all_names = names_df['Male'].to_list() + names_df['Female'].to_list()"],"metadata":{"id":"MEdIok_Vagv7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(data_path + 'NPs/NP_mixed.csv')"],"metadata":{"id":"eYYZfcOfxHyM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f_nps = df['Female'].tolist()\n","m_nps = df['Male'].tolist()"],"metadata":{"id":"rFjbXygfCVu-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(f_nps), len(m_nps)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SmUmjK_hCxA-","executionInfo":{"status":"ok","timestamp":1665953391816,"user_tz":240,"elapsed":5,"user":{"displayName":"Yuewei Yuan","userId":"03266189347381662209"}},"outputId":"6f395897-4c5e-4580-8950-350fa623c9ab"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(40, 40)"]},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","source":["##  permutations"],"metadata":{"id":"G9n0XfzJSPcg"}},{"cell_type":"markdown","source":["### (first time) Generate NP pairs"],"metadata":{"id":"p5qY4R1cKyHd"}},{"cell_type":"code","source":["!pip install iteration-utilities"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sw1veeZqMrbW","executionInfo":{"status":"ok","timestamp":1665953394602,"user_tz":240,"elapsed":2790,"user":{"displayName":"Yuewei Yuan","userId":"03266189347381662209"}},"outputId":"7064a2f4-6356-412d-ecff-13e1ac0ceedd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: iteration-utilities in /usr/local/lib/python3.7/dist-packages (0.11.0)\n"]}]},{"cell_type":"code","source":["from iteration_utilities import random_combination\n","import random\n","import itertools\n","import numpy as np"],"metadata":{"id":"eP9pC8uCMp1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_np_pairs = 26"],"metadata":{"id":"xw1L2OaaVQUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_N3(n1, n2, nps): \n","  # given the two NPs, get a NP for N3 position so that it's different from n1 or n2\n","  n3 = random.choice(nps)\n","  while n3 == n1 or n3 == n2:\n","    n3 = random.choice(nps)\n","  return n3"],"metadata":{"id":"NdcajMSaHKKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","all_2f = set()\n","nps_3f = []\n","while count < num_np_pairs:\n","  f1, f2 = random_combination(f_nps, r=2)\n","  while (f1, f2) in all_2f:\n","    f1, f2 = random_combination(f_nps, r=2)\n","  all_2f.add((f1,f2))\n","  f3 = get_N3(f1, f2, f_nps)\n","  nps_3f.append((f1,f2,f3))\n","  nps_3f.append((f2,f1,f3))\n","  count += 2\n","  if count >= num_np_pairs:\n","    break"],"metadata":{"id":"D-oNw4HWNFOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","all_2m = set()\n","nps_3m = []\n","while count < num_np_pairs:\n","  m1, m2 = random_combination(m_nps, r=2)\n","  while (m1, m2) in all_2m:\n","    m1, m2 = random_combination(m_nps, r=2)\n","  all_2m.add((m1, m2))\n","  m3 = get_N3(m1, m2, m_nps)\n","  nps_3m.append((m1,m2,m3))\n","  nps_3m.append((m2,m1,m3))\n","  count += 2\n","  if count >= num_np_pairs:\n","    break"],"metadata":{"id":"l91QjHDEQUkf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","nps_2f_m = []\n","\n","while count < num_np_pairs:\n","  f1, f2 = random_combination(f_nps, r=2)\n","  while (f1, f2) in all_2f:\n","    f1, f2 = random_combination(f_nps, r=2)\n","  all_2f.add((f1,f2))\n","  m3 = random.choice(m_nps)\n","  nps_2f_m.append((f1,f2,m3))\n","  nps_2f_m.append((f2,f1,m3))\n","  count += 2\n","  if count >= num_np_pairs:\n","    break\n"],"metadata":{"id":"zfvsm3Akj5Ly"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","nps_2m_f = []\n","\n","while count < num_np_pairs:\n","  m1, m2 = random_combination(m_nps, r=2)\n","  while (m1, m2) in all_2m:\n","    m1, m2 = random_combination(m_nps, r=2)\n","  all_2m.add((m1, m2))\n","  f3 = random.choice(f_nps)\n","  nps_2m_f.append((m1,m2,f3))\n","  nps_2m_f.append((m2,m1,f3))\n","  count += 2\n","  if count >= num_np_pairs:\n","    break\n"],"metadata":{"id":"IgGEGMdGq-EE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","nps_2m = []\n","\n","while count < num_np_pairs:\n","  m1, m2 = random_combination(m_nps, r=2)\n","  while (m1, m2) in all_2m:\n","    m1, m2 = random_combination(m_nps, r=2)\n","  all_2m.add((m1, m2))\n","  nps_2m.append((m1,m2))\n","  nps_2m.append((m2,m1))\n","  count += 1\n","  if count >= num_np_pairs:\n","    break"],"metadata":{"id":"6fwQ7zJ8kKEr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["count = 0\n","nps_2f = []\n","\n","while count < num_np_pairs:\n","  f1, f2 = random_combination(f_nps, r=2)\n","  while (f1, f2) in all_2f:\n","    f1, f2 = random_combination(f_nps, r=2)\n","  all_2f.add((f1, f2))\n","  nps_2f.append((f1, f2))\n","  nps_2f.append((f2, f1))\n","  count += 1\n","  if count >= num_np_pairs:\n","    break"],"metadata":{"id":"pQb2eo8CsN9W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### (first time) Save the generated NP pairs - For keeping track"],"metadata":{"id":"VsDv-GgNf71C"}},{"cell_type":"code","source":["# for the purpose of keeping track - making sure we are using the same sets of NP pairs during sentence generation\n","np_pairs_df = pd.DataFrame(list(zip(nps_3m, nps_3f, nps_2m_f, nps_2f_m, nps_2m, nps_2f)),\n","                           columns = ['nps_3m', 'nps_3f', 'nps_2m_f', 'nps_2f_m', 'nps_2m', 'nps_2f'])\n","np_pairs_df.to_csv(data_path+'NPs/All_generated_NP_pairs.csv')"],"metadata":{"id":"99zqayX-KtXm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nps_3m_df = pd.DataFrame(nps_3m, columns=['m1','m2','m3'])\n","nps_3f_df = pd.DataFrame(nps_3f, columns=['f1','f2','f3'])\n","nps_2m_f_df = pd.DataFrame(nps_2m_f, columns=['m1','m2','f'])\n","nps_2f_m_df = pd.DataFrame(nps_2f_m, columns=['f1','f2','m'])\n","nps_2m_df = pd.DataFrame(nps_2m, columns=['m1','m2'])\n","nps_2f_df = pd.DataFrame(nps_2f, columns=['f1','f2'])"],"metadata":{"id":"0toswGcdeMTr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nps_dfs = [nps_3m_df, nps_3f_df, nps_2m_f_df, nps_2f_m_df, nps_2m_df, nps_2f_df]\n","nps_dfs_names = ['nps_3m', 'nps_3f', 'nps_2m_f', 'nps_2f_m', 'nps_2m', 'nps_2f']\n","for i in range(len(nps_dfs)):\n","  nps_dfs[i].to_csv(data_path+'NPs/'+nps_dfs_names[i]+'.csv')"],"metadata":{"id":"2hMKzKB2fBtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Retrieve the NP pairs"],"metadata":{"id":"u5qTK0LnGbdc"}},{"cell_type":"code","source":["# nps_3m_df = pd.read_csv(data_path+'NPs/nps_3m.csv')\n","# nps_3f_df = pd.read_csv(data_path+'NPs/nps_3f.csv')\n","# nps_2m_f_df = pd.read_csv(data_path+'NPs/nps_2m_f.csv')\n","# nps_2f_m_df = pd.read_csv(data_path+'NPs/nps_2f_m.csv')\n","# nps_2m_df = pd.read_csv(data_path+'NPs/nps_2m.csv')\n","# nps_2f_df = pd.read_csv(data_path+'NPs/nps_2f.csv')\n"],"metadata":{"id":"uuWMtgNfPu_X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IM5t--IrUOOL"},"source":["## Verbs\n"]},{"cell_type":"code","metadata":{"id":"pGe_iRd5rmtQ"},"source":["# Type ECO\n","ECO_all = pd.read_csv(data_path+\"verb_phrases/ECO.csv\")\n","amuse_verbs = ECO_all['bored'].dropna()\n","see_verbs = ECO_all['saw'].dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Type ECS\n","ECS_all = pd.read_csv(data_path+\"verb_phrases/ECS.csv\")\n","admire_verbs = ECS_all['liked'].dropna()\n","meet_verbs = ECS_all['met-with'].dropna()"],"metadata":{"id":"t69FmtA_OhcX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Type IC\n","IC_all = pd.read_csv(data_path+\"verb_phrases/IC.csv\")\n","call_verbs = IC_all['called'].dropna()\n","reasons_amb = IC_all['reason_ambig'].dropna()\n","reasons_unamb = IC_all['reason_unambig'].dropna()"],"metadata":{"id":"1hKo1liYvI4Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Type TOP\n","TOP_all = pd.read_csv(data_path+\"verb_phrases/TOP.csv\")\n","\n","def male_to_female(text):\n","  text1 = text.replace(\" he \",\" she \")\n","  text2 = text1.replace(\" him\", \" her\")\n","  text3 = text2.replace(\" his\", \" her\")\n","  return text3"],"metadata":{"id":"rWeiULC03bSf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vPKmzJepUvNO"},"source":["# Type ECO"]},{"cell_type":"code","source":["def grammar(np_pair):\n","  np_pair_g = []\n","  for i in range(len(np_pair)):\n","    if np_pair[i] in all_names:\n","      np_pair_g.append(np_pair[i])\n","    elif i == 0:\n","      np_pair_g.append(\"The \" + np_pair[i])\n","    else:\n","      np_pair_g.append(\"the \" + np_pair[i])\n","  return np_pair_g"],"metadata":{"id":"knOCKPqGgLmg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCBwmFNkS92J"},"source":["##Unambiguous ECO-1: [Name A] told [Name B] that [pronoun] [saw] [Name C].\n"]},{"cell_type":"code","metadata":{"id":"vgwMhMqmS92K"},"source":["import random\n","import itertools\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECO-1_unambiguous.txt\",\"w+\")\n","\n","for s in see_verbs:\n","  #f1 told f2 that she saw f3 (f3!=f1, f3!=f2, f1!=f2)\n","  for p in nps_3f:\n","    f1, f2, f3 = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that she \" + s + \" \" + f3  +  \".\\n\") \n","\n","  #f1 told f2 that she saw m\n","  for p in nps_2f_m:\n","      f1, f2, m = grammar(p)\n","      file_a.write(f1 + \" told \" + f2 + \" that she \" + s + \" \" + m  +  \".\\n\") \n","      \n","  #m1 told m2 that he saw m3 (m3!=m1, m3!=m2, m1!=m2)\n","  for p in nps_3m:\n","    m1, m2, m3 = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that he \" + s + \" \" + m3  +  \".\\n\") \n","    \n","  #m1 told m2 that she saw f\n","  for p in nps_2m_f:\n","    m1, m2, f = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that he \" + s + \" \" + f  +  \".\\n\")\n","\n","file_a.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFm4AQOtIGS9"},"source":["## Ambiguous ECO-1: [Name A] told [Name B] that [pronoun] [bored] [Name C]."]},{"cell_type":"code","metadata":{"id":"uZywnPxKIGS-"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECO-1_ambiguous.txt\",\"w+\")\n","\n","for s in amuse_verbs:\n","  #f1 told f2 that she bored f3 (f3!=f1, f3!=f2, f1!=f2)\n","  for p in nps_3f:\n","    f1,f2,f3 = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that she \" + s + \" \" + f3  +  \".\\n\") \n","\n","\n","  #f1 told f2 that she bored m\n","  for p in nps_2f_m:\n","      f1,f2,m = grammar(p)\n","      file_a.write(f1 + \" told \" + f2 + \" that she \" + s + \" \" + m  +  \".\\n\") \n","\n","  #m1 told m2 that he bored m3 (m3!=m1, m3!=m2, m1!=m2)\n","  for p in nps_3m:\n","    m1,m2,m3 = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that he \" + s + \" \" + m3  +  \".\\n\") \n","\n","  #m1 told m2 that she bored f\n","  for p in nps_2m_f:\n","    m1,m2,f = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that he \" + s + \" \" + f  +  \".\\n\") \n","\n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6C2fNerS1ibn"},"source":["##Unambiguous ECO-2: [Name A] told [Name B] that [pronoun] [saw] the client.\n"]},{"cell_type":"code","metadata":{"id":"IhqtcU0J1ibx"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECO-2_unambiguous_.txt\",\"w+\")\n","\n","for s in see_verbs:\n","  #f1 told f2 that she saw <their colleagues>\n","  for p in nps_2f:\n","      f1,f2 = grammar(p) \n","      file_a.write(f1 + \" told \" + f2 + \" that she \" + s + \" the client.\\n\") \n","\n","  #m1 told m2 that he saw <their colleagues>\n","  for p in nps_2m:\n","    m1,m2 = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that he \" + s + \" the client.\\n\") \n","\n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dRpDiu0XhD8"},"source":["\n","## Ambiguous ECO-2: [Name A] told [Name B] that [pronoun] [bored] the client."]},{"cell_type":"code","metadata":{"id":"k4c9sSR1OQyv"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECO-2_ambiguous.txt\",\"w+\")\n","\n","for v in amuse_verbs:\n","  #f1 told f2 that she bored <their colleagues>\n","  for p in nps_2f:\n","    f1,f2 = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that she \" + v + \" the client.\\n\") \n","\n","  #m1 told m2 that he bored <their colleagues>\n","  for p in nps_2m:\n","    m1,m2 = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that he \" + v + \" the client.\\n\") \n","\n","\n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"deavivptPpHA"},"source":["# Type ECS"]},{"cell_type":"markdown","metadata":{"id":"Muyv6SP2fzs7"},"source":["## Unambiguous ECS-1: [Name A] [told] [Name B] that [Name C] [met with] [pronoun].\n"]},{"cell_type":"code","metadata":{"id":"D2cJfa4PfztB"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECS-1_unambiguous.txt\",\"w+\")\n","\n","\n","for s in meet_verbs:\n","  #f1 told f2 that f3 met with her (f3!=f1, f3!=f2, f1!=f2)\n","  for p in nps_3f:\n","    f1,f2,f3 = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that \" + f3 + \" \" + s + \" her.\\n\") \n","\n","\n","  #f1 told f2 that m met with her\n","  for p  in nps_2f_m:\n","    f1,f2,m = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that \" + m + \" \" + s + \" her.\\n\") \n","    \n","  #m1 told m2 that m3 met with him (m3!=m1, m3!=m2, m1!=m2)\n","  for p in nps_3m:\n","    m1,m2,m3 = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that \" + m3 + \" \" + s + \" him.\\n\")\n","    \n","  #m1 told m2 that f met with him\n","  for p in nps_2m_f:\n","    m1,m2,f = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that \" + f + \" \" + s + \" him.\\n\")\n","    \n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seTWN36TPpHN"},"source":["## Ambiguous ECS-1: [Name A] [told] [Name B] that [Name C] [liked] [pronoun].\n"]},{"cell_type":"code","metadata":{"id":"UYIwugLQPpHN"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECS-1_ambiguous.txt\",\"w+\")\n","\n","for s in admire_verbs:\n","  #f1 told f2 that f3 liked her (f3!=f1, f3!=f2, f1!=f2)\n","  for p in nps_3f:\n","    f1,f2,f3 = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that \" + f3 + \" \" + s + \" her.\\n\") \n","    \n","  #f1 told f2 that m liked her\n","  for p in nps_2f_m:\n","    f1,f2,m = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that \" + m + \" \" + s + \" her.\\n\") \n","    \n","  #m1 told m2 that m3 liked him (m3!=m1, m3!=m2, m1!=m2)\n","  for p in nps_3m:\n","    f1,f2,m = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that \" + m3 + \" \" + s + \" him.\\n\") \n","    \n","  #m1 told m2 that f liked him\n","  for p in nps_2m_f:\n","    m1,m2,f = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that \" + f + \" \" + s + \" him.\\n\")\n","    \n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9EAfe-DDdMCA"},"source":["## Unambiguous ECS-2: [Name A] [told] [Name B] that the client [met with] [pronoun].\n"]},{"cell_type":"code","metadata":{"id":"aDooLBc1dMCF"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECS-2_unambiguous.txt\",\"w+\")\n","\n","for s in meet_verbs:\n","\n","  #f1 told f2 that the client met with her (f3!=f1, f3!=f2, f1!=f2)\n","  for p in nps_2f:\n","    f1, f2 = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that the client \" + s + \" her.\\n\")\n","\n","  #m1 told m2 that the client met with him\n","  for p in nps_2m:\n","    m1, m2 = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that the client \" + s + \" him.\\n\") \n","\n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5glPdXcxPpHI"},"source":["## Ambiguous ECS-2: [Name A] [told] [Name B] that the client [liked] [pronoun].\n"]},{"cell_type":"code","metadata":{"id":"0OKJaa49PpHK"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/ECS-2_ambiguous_active.txt\",\"w+\")\n","\n","for s in admire_verbs:\n","\n","  #f1 told f2 that the client met with her (f3!=f1, f3!=f2, f1!=f2)\n","  for p in nps_2f:\n","    f1, f2 = grammar(p)\n","    file_a.write(f1 + \" told \" + f2 + \" that the client \" + s + \" her.\\n\") \n","    \n","  #m1 told m2 that the client met with him\n","  for p in nps_2m:\n","    m1, m2 = grammar(p)\n","    file_a.write(m1 + \" told \" + m2 + \" that the client \" + s + \" him.\\n\") \n","    \n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ojzzco9WjxUm"},"source":["# Type IC"]},{"cell_type":"markdown","source":["##Unambiguous"],"metadata":{"id":"wWCY2Gn20PVE"}},{"cell_type":"code","metadata":{"id":"uzmHaGM-0PVF"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/IC_unambiguous.txt\",\"w+\")\n","\n","for i in range(len(call_verbs)):\n","  c = call_verbs[i]\n","  c_p = call_passive[i]\n","  for r in reasons_unamb: \n","    for p in nps_2m:\n","      m1, m2 = grammar(p)\n","      file_a.write(m1 + \" \" + c + \" \" + m2 + \" because he \" + r + \".\\n\")\n","\n","    for p in nps_2f:\n","      f1, f2 = grammar(p)\n","      file_a.write(f1 + \" \" + c + \" \" + f2 + \" because she \" + r + \".\\n\")\n","\n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Ambiguous"],"metadata":{"id":"pEceKdw9xV_L"}},{"cell_type":"code","metadata":{"id":"7Pi9Ws-vjxUm"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/data_ealc/sentences/IC_ambiguous.txt\",\"w+\")\n","\n","for i in range(len(call_verbs)):\n","  c = call_verbs[i]\n","  \n","  for r in reasons_amb: \n","    for p in nps_2m:\n","      m1, m2 = grammar(p)\n","      file_a.write(m1 + \" \" + c + \" \" + m2 + \" because he \" + r + \".\\n\")\n","      \n","    for p in nps_2f:\n","      f1, f2 = grammar(p)\n","      file_a.write(f1 + \" \" + c + \" \" + f2 + \" because she \" + r + \".\\n\")\n","      \n","file_a.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qk-rfskLXU5B"},"source":["# Type TOP"]},{"cell_type":"markdown","source":["##Unambiguous"],"metadata":{"id":"KvbJxIb1XU5D"}},{"cell_type":"code","metadata":{"id":"i_dYscVwXU5E"},"source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/Data/sentences/TOP_unambiguous.txt\",\"w+\")\n","\n","for i,row in TOP_all.iterrows():\n","  verb_m = row['event_unambig']\n","  verb_f = male_to_female(verb_m)\n","\n","  for p in nps_2f:\n","    f1,f2 = grammar(p)\n","    file_a.write(f1 + \" \" + row['passed'] + \" \" + f2 + \" \" + row['DO'] + \" \" + row['prep'] + \" she \" + verb_f + \".\\n\")\n","    \n","  for p in nps_2m:\n","    m1,m2 = grammar(p)\n","    file_a.write(m1 + \" \"+ row['passed'] + \" \" + m2 + \" \" + row['DO'] + \" \" + row['prep'] + \" he \" + verb_m + \".\\n\")\n","    \n","file_a.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Ambiguous"],"metadata":{"id":"afFNYhcZXU5B"}},{"cell_type":"code","source":["import random\n","\n","file_a = open(r\"/content/drive/My Drive/AmbiCoref/Data/sentences/TOP_ambiguous.txt\",\"w+\")\n","\n","for i,row in TOP_all.iterrows():\n","  verb_m = row['event_ambig']\n","  verb_f = male_to_female(verb_m)\n","\n","  for p in nps_2f:\n","    f1,f2 = grammar(p)\n","    file_a.write(f1 + \" \"+ row['passed'] + \" \" + f2 + \" \" + row['DO'] + \" \" + row['prep'] + \" she \" + verb_f + \".\\n\")\n","    \n","  for p in nps_2m:\n","    m1,m2 = grammar(p)\n","    file_a.write(m1 + \" \"+ row['passed'] + \" \" + m2 + \" \" + row['DO'] + \" \" + row['prep'] + \" he \" + verb_m + \".\\n\")\n","      \n","file_a.close()"],"metadata":{"id":"ZPqPyfWCGIpN"},"execution_count":null,"outputs":[]}]}